{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing modules that are needed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as f \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameters for the problem and architecture\n",
    "input_size = 576\n",
    "latent_dim = 32\n",
    "torch.manual_seed(128)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "L = 24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = pd.read_csv(\\\n",
    "    '~/ml_J1-J2_supervised/all_phase/af/augumented_dataL24.csv',index_col=[0])\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(data_f.iloc[:,:-1],data_f.iloc[:,-1:], \\\n",
    "                    random_state=42,test_size=0.2,stratify=data_f.iloc[:,-1:])\n",
    "\n",
    "data_train = pd.concat([X_train,y_train],axis=1)\n",
    "data_test = pd.concat([X_test,y_test],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Class definition that is used to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### class to load the data\n",
    "class LoadData(Dataset):\n",
    "    def __init__(self,data,L,device=device):\n",
    "        self.L = L\n",
    "        self.x_data = torch.tensor(data.iloc[:,:-1].values,dtype=torch.float32).to(device=device)\n",
    "        self.y_data = torch.tensor(data.iloc[:,-1:].values,dtype=torch.long).to(device=device)\n",
    "\n",
    "    ### length of the dataset\n",
    "    ### function one has to use if you want to define a custom dataset class\n",
    "    def __len__(self):\n",
    "        return len(self.y_data)\n",
    "\n",
    "    \n",
    "    ## get the image and label\n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.to_list()\n",
    "        \n",
    "        image = self.x_data[idx,:]\n",
    "        label = self.y_data[idx]\n",
    "\n",
    "        return {'data':image,'label':label}\n",
    "\n",
    "train_dataloader = LoadData(data_train,L,device=device)\n",
    "test_dataloader = LoadData(data_test,L,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Model definition for the variational autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### definition of the network\n",
    "class encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,latent_dim):\n",
    "        super(encoder,self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_size = input_size\n",
    "        ### encoder network\n",
    "        self.encoder1 = nn.Linear(in_features=self.input_size,out_features=256)\n",
    "        self.encoder2 = nn.Linear(in_features=256,out_features=128)\n",
    "        self.encoder3 = nn.Linear(in_features=128,out_features=64)\n",
    "        self.encoder4 = nn.Linear(in_features=64,out_features=32)\n",
    "\n",
    "        ### layer for mu_x and simga_x\n",
    "        self.mux = nn.Linear(in_features=32,out_features=self.latent_dim)\n",
    "        self.varx = nn.Linear(in_features=32,out_features=self.latent_dim)\n",
    "\n",
    "        ## normal distribution       \n",
    "        self.N = torch.distributions.Normal(0,1)\n",
    "\n",
    "        self.kl = 0\n",
    "\n",
    "    ### feedforward network \n",
    "    def forward(self,x):\n",
    "        ##  passing the input through the network\n",
    "        x = self.encoder1(x)\n",
    "        x = self.encoder2(x)\n",
    "        x = self.encoder3(x)\n",
    "        x = self.encoder4(x)\n",
    "\n",
    "        ## passing input x to layer mu and var, mux = g(x), sigmax = f(x) \n",
    "        mu = self.mux(x)\n",
    "        sigma = torch.exp(self.varx(x))\n",
    "\n",
    "        ## combining mu and sigma to a normal distribution reparameterization trick\n",
    "        ##  z = mu + sigma * N(0,1)\n",
    "        zi = mu + sigma * self.N.sample(sigma.shape)\n",
    "        \n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1./2).sum()\n",
    "        return zi\n",
    "\n",
    "\n",
    "\n",
    "### definition of the network\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, latent_dim):\n",
    "        super(decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_size = input_size\n",
    "       \n",
    "        ### decoder network\n",
    "        self.decoder1 = nn.Linear(in_features=self.latent_dim, out_features=32)\n",
    "        self.decoder2 = nn.Linear(in_features=32, out_features=64)\n",
    "        self.decoder3 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.decoder4 = nn.Linear(in_features=128, out_features=self.input_size)\n",
    "\n",
    "    ### feedforward network\n",
    "    def forward(self, x):\n",
    "        ##  passing the input through the network\n",
    "       \n",
    "        x = self.decoder1(x)\n",
    "        x = self.decoder2(x)\n",
    "        x = self.decoder3(x)\n",
    "        x = self.decoder4(x)\n",
    "\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "### class to implement encoder-decoder network\n",
    "### it used encoder-decoder classes defined previouly\n",
    "class VariationalAutoencd(nn.Module):\n",
    "    def __init__(self,input_size,latent_dim):\n",
    "        super(VariationalAutoencd,self).__init__()\n",
    "        self.encoder = encoder(input_size,latent_dim)\n",
    "        self.decoder = decoder(input_size,latent_dim)\n",
    "    \n",
    "    ### feedforward network\n",
    "    def forward(self,x):\n",
    "        z = self.encoder(x)\n",
    "        xp = self.decoder(z)\n",
    "        return xp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps of the process\n",
    "* Pass the input $X$ through the encoder stage $x_enc$\n",
    "* To get estimate of $\\mu$ and $\\sigma$ use two neural network and pass $x_enc$ through both of them\n",
    "    * $\\mu_{x} = f_{2}(f_{1}(x))$\n",
    "    * $\\sigma_{x} = g_{2}(f_{1}(x))$\n",
    "    * Network $f_{1}(x)$ represents the input that is passed through various stages (neural network) and $f_{2},g_{2}$ represents two different neural network.\n",
    "* To generate the distribution $q(z|x)$ use $\\mu_{x},\\sigma_{x}$ to generate a normal distribution.\n",
    "    * $q(z|x) = \\mathcal{N}(\\mu_{x},\\sigma_{x})$\n",
    "    * Sample a point from this distribution $q(z|x)$\n",
    "    * Directly sampling from $q(z|x)$ is not possible\n",
    "    * However this doesn't allow for back progagation so we use reparameterization trick\n",
    "        * $z \\sim  \\mu_{x} + \\sigma_{x} \\mathcal{N}(0,I)$\n",
    "* Now using $z$ we have to generate $p(x|z)$.\n",
    "* The error one is trying to minimize is \n",
    "\\begin{equation}\n",
    "min E_{q} \\left [ \\log q(z|x) - \\log p(z) \\right ] -  {E}_{q} \\log p(x|z) \n",
    "\\end{equation}\n",
    "* Here $E_{q}$ means average value for a given distribution of $q(z|x)$\n",
    "* Distribution $p(z) = \\mathcal{N} (0,1)$ is a standard normal.\n",
    "* Distribution $p(x|z) = \\mathcal{N}(f(z),cI) = \\mathcal{N} (decoder(z),cI)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define the training function\n",
    "def train(model,device,dataloader,optimizer):\n",
    "    ## set train mode for encoder and decoder\n",
    "    model.train()\n",
    "    loss_train = 0.0 \n",
    "    for batch_idx,batch in enumerate(dataloader):\n",
    "        x=batch['data'].to(device=device)\n",
    "\n",
    "        ## pass the data through the network\n",
    "        xnew = model(x)\n",
    "\n",
    "        ## calculate the loss (reconstruction loss) and (kl divergence)\n",
    "        loss = model.encoder.kl + ((x-xnew)**2).sum()\n",
    "\n",
    "        ## backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "    print(loss_train/len(dataloader.dataset))\n",
    "    return loss_train/len(dataloader.dataset)\n",
    "\n",
    "### define the training function\n",
    "def test(model, device, dataloader, optimizer):\n",
    "    ## set train mode for encoder and decoder\n",
    "    model.eval()\n",
    "\n",
    "    loss_test = 0.0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        x = batch['data'].to(device=device)\n",
    "\n",
    "        ## pass the data through the encoder network\n",
    "        z = model.encoder(x)\n",
    "\n",
    "        ## get the reconstructed data by passing the data through the network\n",
    "        xp = model(x)\n",
    "\n",
    "        ## calculate the loss (reconstruction loss) and (kl divergence)\n",
    "        loss = model.encoder.kl + ((x-xp)**2).sum()\n",
    "    \n",
    "        loss_test += loss.item()\n",
    "    print(loss_test/len(dataloader.dataset))\n",
    "    return loss_test/len(dataloader.dataset) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = torch.randn(input_size)\n",
    "\n",
    "### create an instance of variational autoencoder\n",
    "vae = VariationalAutoencd(input_size, latent_dim)\n",
    "\n",
    "### set the optimizer with the necessary parameter\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(vae,device,train_dataloader,optimizer)\n",
    "    test_loss = train(vae,device,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "142c5f550c5dca9f67c8c920311c6ac8b8cbcb1ffb5f20302b2d142b28b10006"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('python38-demo-v2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
